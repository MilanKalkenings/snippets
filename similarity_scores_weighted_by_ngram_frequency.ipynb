{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npip install kagglehub==0.3.3\\npip install scikit-learn==1.5.2\\npip install textdistance==4.6.2\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "pip install kagglehub==0.3.3\n",
    "pip install scikit-learn==1.5.2\n",
    "pip install textdistance==4.6.2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create negatives\n",
    "def create_negatives(df: pd.DataFrame) -> pd.Series:\n",
    "    while True:\n",
    "        index_shuffled = pd.Series(df.index).sample(frac=1).values\n",
    "        if (index_shuffled == df.index).sum() == 0:\n",
    "            break\n",
    "    neg = df[\"description_y\"][index_shuffled]\n",
    "    neg.index = df.index \n",
    "\n",
    "    df[\"same\"] = 1\n",
    "    df_neg = pd.concat([df[\"description_x\"], neg], axis=1)\n",
    "    df_neg[\"same\"] = 0\n",
    "    return pd.concat([df, df_neg]).sample(frac=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Tuple\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import textdistance\n",
    "import re\n",
    "\n",
    "\n",
    "class WeightedSims:\n",
    "    def __init__(self):\n",
    "        self.whitespace_pattern = re.compile(r\"\\s+\")\n",
    "        self.digits_pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "    def rescale(self, ser: pd.Series, min: float, max: float = 1) -> pd.Series:\n",
    "        return min + (ser - ser.min()) * (max - min) / (ser.max() - ser.min())\n",
    "\n",
    "    def fit(self, texts_train: pd.Series, texts_test: pd.Series, min_weight: float):\n",
    "        \"\"\"\n",
    "        for simplicity assumes that test data is not streamed, but completely given\n",
    "        - train data is used to learn/assign weights to characters\n",
    "        - test data is merely used to assign a naive \"neutral\" weight of 1 to characters that don't appear in train data\n",
    "        \"\"\"\n",
    "        count_vectorizer = CountVectorizer(analyzer='char')\n",
    "        char_counts = count_vectorizer.fit_transform(texts_train)\n",
    "        char_counts_df = pd.DataFrame(char_counts.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "        char_counts_rel = (char_counts_df.sum() / char_counts_df.sum().sum()).sort_values(ascending=False)\n",
    "        # treat chars that only appear in test as absolutely rare\n",
    "        self.weights = (1 - char_counts_rel)\n",
    "        self.weights = self.rescale(self.weights, min=min_weight)\n",
    "        chars_test = list(set(texts_test.str.cat()))\n",
    "        for char in [c for c in chars_test if c not in self.weights.index]:\n",
    "            self.weights.loc[char] = 1\n",
    "\n",
    "        vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 3), norm=None, binary=False)\n",
    "        ngrams_count = pd.DataFrame(vectorizer.fit_transform(texts_train).toarray(), columns=vectorizer.get_feature_names_out())\n",
    "        self.weights_3gram = (1 - (ngrams_count.sum() / ngrams_count.sum().sum()).sort_values())\n",
    "        self.weights_3gram = self.rescale(self.weights_3gram, min=min_weight)\n",
    "        # treat ngrams that only appear in test as absolutely rare\n",
    "        vectorizer.fit(texts_test)\n",
    "        ngrams_test = vectorizer.get_feature_names_out()\n",
    "        for ngram in ngrams_test:\n",
    "            if ngram not in self.weights_3gram.index:\n",
    "                self.weights_3gram.loc[ngram] = 1\n",
    "\n",
    "        vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 2), norm=None, binary=False)\n",
    "        ngrams_count = pd.DataFrame(vectorizer.fit_transform(texts_train).toarray(), columns=vectorizer.get_feature_names_out())\n",
    "        self.weights_2gram = (1 - (ngrams_count.sum() / ngrams_count.sum().sum()).sort_values())\n",
    "        self.weights_2gram = self.rescale(self.weights_2gram, min=min_weight)\n",
    "        # treat ngrams that only appear in test as absolutely rare\n",
    "        vectorizer.fit(texts_test)\n",
    "        ngrams_test = vectorizer.get_feature_names_out()\n",
    "        for ngram in ngrams_test:\n",
    "            if ngram not in self.weights_2gram.index:\n",
    "                self.weights_2gram.loc[ngram] = 1\n",
    "\n",
    "    def jaccard_char_level(self, text1: str, text2: str) -> float:\n",
    "        if len(text1) == 0 or len(text2) == 0:\n",
    "            return 0.\n",
    "        elif text1 == text2:\n",
    "            return 1.\n",
    "        union_ids = list(set([c for c in text1] + [c for c in text2]))\n",
    "        intersection = self.weights[list(set([c for c in text1 if c in text2] + [c for c in text2 if c in text1]))]\n",
    "        union = self.weights[union_ids]\n",
    "        return float(intersection.sum() / union.sum())\n",
    "    \n",
    "    def jaccard_initials(self, text1: str, text2: str) -> float:\n",
    "        initials_text1 = \"\".join([w[0] for w in re.split(pattern=self.whitespace_pattern, string=text1)])\n",
    "        initials_text2 = \"\".join([w[0] for w in re.split(pattern=self.whitespace_pattern, string=text2)])\n",
    "        return self.jaccard_char_level(text1=initials_text1, text2=initials_text2)\n",
    "    \n",
    "    def jaccard_digits(self, text1: str, text2: str) -> float:\n",
    "        digits_text1 = \"\".join(re.findall(pattern=self.digits_pattern, string=text1))\n",
    "        digits_text2 = \"\".join(re.findall(pattern=self.digits_pattern, string=text2))\n",
    "        return self.jaccard_char_level(digits_text1, digits_text2)\n",
    "    \n",
    "    def hamming(self, text1: str, text2: str, overshoot_weight: float = 0.5, sort: bool = False) -> float:\n",
    "        if len(text1) == 0 and len(text2) == 0:\n",
    "            return 0.\n",
    "        elif text1 == text2:\n",
    "            return 1.\n",
    "        # sort\n",
    "        if sort:\n",
    "            text1 = \" \".join(sorted(re.split(pattern=self.whitespace_pattern, string=text1)))\n",
    "            text2 = \" \".join(sorted(re.split(pattern=self.whitespace_pattern, string=text2)))\n",
    "        # determine long and short text\n",
    "        if len(text1) >= len(text2):\n",
    "            text_long = text1\n",
    "            text_short = text2\n",
    "        else:\n",
    "            text_long = text2\n",
    "            text_short = text1\n",
    "        # calculate weighted similarity\n",
    "        total = 0\n",
    "        sim = 0\n",
    "        for position in range(len(text_long)):\n",
    "            if position < len(text_short):\n",
    "                total += max(self.weights[text_long[position]], self.weights[text_short[position]])\n",
    "                if text_short[position] == text_long[position]:\n",
    "                    sim += self.weights[text_short[position]]\n",
    "            else:\n",
    "                total += self.weights[text_long[position]] * overshoot_weight\n",
    "        return sim / total\n",
    "    \n",
    "    def longest_common_substring(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"\n",
    "        hello, he eats -> he\n",
    "        \"\"\"\n",
    "        weight1 = self.weights.loc[[c for c in text1]].sum()\n",
    "        weight2 = self.weights.loc[[c for c in text2]].sum()\n",
    "        weight_total = max(weight1, weight2)\n",
    "\n",
    "        common_substring = textdistance.lcsstr(text1, text2)\n",
    "        weight_common = self.weights.loc[[c for c in common_substring]].sum()\n",
    "        return float(weight_common / weight_total)\n",
    "    \n",
    "    def longest_common_subseq(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"\n",
    "        hello, he will -> hell (longest common substring with skipping)\n",
    "        \"\"\"\n",
    "        weight1 = self.weights.loc[[c for c in text1]].sum()\n",
    "        weight2 = self.weights.loc[[c for c in text2]].sum()\n",
    "        weight_total = max(weight1, weight2)\n",
    "\n",
    "        common_subsseq = textdistance.lcsseq(text1, text2)\n",
    "        weight_common = self.weights.loc[[c for c in common_subsseq]].sum()\n",
    "        return float(weight_common / weight_total)\n",
    "    \n",
    "    def ngrams(self, text1: str, text2: str, n: int = 3) -> float:\n",
    "        if len(text1) == 0 or len(text2) == 0:\n",
    "            return 0.\n",
    "        elif text1 == text2:\n",
    "            return 1.\n",
    "        \n",
    "        if n == 3:\n",
    "            weights = self.weights_3gram\n",
    "        elif n == 2:\n",
    "            weights = self.weights_2gram\n",
    "        \n",
    "        ngrams1 = [text1[i:i+n] for i in range(len(text1) - n + 1)]\n",
    "        ngrams2 = [text2[i:i+n] for i in range(len(text2) - n + 1)]\n",
    "        union = list(set(ngrams1 + ngrams2))\n",
    "        intersection = [n for n in ngrams1 if n in ngrams2]\n",
    "        weights_union = weights[union].sum()\n",
    "        weights_intersecion = weights[intersection].sum()\n",
    "        return weights_intersecion / weights_union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def add_features(df: pd.DataFrame, weighted_sims):\n",
    "    df[\"2gram\"] = df.apply(lambda row: weighted_sims.ngrams(row[\"description_x\"], row[\"description_y\"], 2), axis=1)\n",
    "    df[\"3gram\"] = df.apply(lambda row: weighted_sims.ngrams(row[\"description_x\"], row[\"description_y\"], 3), axis=1)\n",
    "    df[\"damerau_levenshtein\"] = df.apply(lambda row: textdistance.damerau_levenshtein(row[\"description_x\"], row[\"description_y\"]) / max(len(row[\"description_x\"]), len(row[\"description_y\"])), axis=1)\n",
    "    \"\"\"\n",
    "    df[\"levenshtein\"] = df.apply(lambda row: textdistance.levenshtein(row[\"description_x\"], row[\"description_y\"]) / max(len(row[\"description_x\"]), len(row[\"description_y\"])), axis=1)\n",
    "    df[\"lcsseq_unweighted\"] = df.apply(lambda row: len(textdistance.lcsseq(row[\"description_x\"], row[\"description_y\"])) / max(len(row[\"description_x\"]), len(row[\"description_y\"])), axis=1)\n",
    "    df[\"lcsseq\"] = df.apply(lambda row: weighted_sims.longest_common_subseq(row[\"description_x\"], row[\"description_y\"]), axis=1)\n",
    "    df[\"lcsstr_unweighted\"] = df.apply(lambda row: len(textdistance.lcsstr(row[\"description_x\"], row[\"description_y\"])) / max(len(row[\"description_x\"]), len(row[\"description_y\"])), axis=1)\n",
    "    df[\"lcsstr\"] = df.apply(lambda row: weighted_sims.longest_common_substring(row[\"description_x\"], row[\"description_y\"]), axis=1)\n",
    "    df[\"cosine\"] = df.apply(lambda row: textdistance.cosine(row[\"description_x\"], row[\"description_y\"]), axis=1)\n",
    "    df[\"jaccard_unweighted\"] = df.apply(lambda row: textdistance.jaccard(row[\"description_x\"], row[\"description_y\"]), axis=1)\n",
    "    df[\"jaccard_char_level\"] = df.apply(lambda row: weighted_sims.jaccard_char_level(row[\"description_x\"], row[\"description_y\"]), axis=1)\n",
    "    df[\"jaccard_initials\"] = df.apply(lambda row: weighted_sims.jaccard_initials(row[\"description_x\"], row[\"description_y\"]), axis=1)\n",
    "    df[\"hamming\"] = df.apply(lambda row: weighted_sims.hamming(row[\"description_x\"], row[\"description_y\"]), axis=1)\n",
    "    \"\"\"\n",
    "    weights_original = copy.deepcopy(weighted_sims.weights)\n",
    "    weighted_sims.weights = pd.Series(index=weighted_sims.weights.index, data=[1] * len(weighted_sims.weights))\n",
    "    \"\"\"\n",
    "    df[\"jaccard_initials_unweighted\"] = df.apply(lambda row: weighted_sims.jaccard_initials(row[\"description_x\"], row[\"description_y\"]), axis=1)\n",
    "    df[\"hamming_unweighted\"] = df.apply(lambda row: weighted_sims.hamming(row[\"description_x\"], row[\"description_y\"]), axis=1)\n",
    "    \"\"\"\n",
    "    df[\"2gram_unweighted\"] = df.apply(lambda row: weighted_sims.ngrams(row[\"description_x\"], row[\"description_y\"], 2), axis=1)\n",
    "    df[\"3gram_unweighted\"] = df.apply(lambda row: weighted_sims.ngrams(row[\"description_x\"], row[\"description_y\"], 3), axis=1)\n",
    "    weighted_sims.weights = weights_original\n",
    "    return df\n",
    "\n",
    "\n",
    "def eval(features: List[str], df_train: pd.DataFrame, df_test: pd.DataFrame):\n",
    "    cls = RandomForestClassifier(min_samples_split=3)\n",
    "    cls.fit(X=df_train[features], y=df_train[\"same\"])\n",
    "    pred_test = cls.predict(X=df_test[features])\n",
    "    return accuracy_score(y_true=df_test[\"same\"], y_pred=pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# download data\n",
    "df_path = kagglehub.dataset_download(\"rishisankineni/text-similarity\")\n",
    "df_train_orig = pd.read_csv(df_path + \"/train.csv\")\n",
    "df_train_orig = df_train_orig[[\"description_x\", \"description_y\"]].drop_duplicates()\n",
    "df_test_orig = pd.read_csv(df_path + \"/test.csv\")\n",
    "df_test_orig = df_test_orig[[\"description_x\", \"description_y\"]].drop_duplicates()\n",
    "\n",
    "accs = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    accs_inner = []\n",
    "    df_train = create_negatives(df_train_orig)\n",
    "    df_test = create_negatives(df_test_orig)\n",
    "    weighted_sims = WeightedSims()\n",
    "    weighted_sims.fit(\n",
    "        texts_train=pd.concat([df_train[\"description_x\"], df_train[\"description_y\"]]), \n",
    "        texts_test=pd.concat([df_test[\"description_x\"], df_test[\"description_y\"]]),\n",
    "        min_weight=0.9)\n",
    "    df_train = add_features(df_train, weighted_sims=weighted_sims)\n",
    "    df_test = add_features(df_test, weighted_sims=weighted_sims)\n",
    "    feats = [c for c in df_train.columns if c not in [\"description_x\", \"description_y\", \"same\"]]\n",
    "    for f in feats:\n",
    "        accs_inner.append(eval(features=[f], df_train=df_train, df_test=df_test))\n",
    "    accs.append(accs_inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "damerau_levenshtein    0.951647\n",
       "3gram_unweighted       0.939535\n",
       "3gram                  0.939341\n",
       "2gram                  0.938953\n",
       "2gram_unweighted       0.938953\n",
       "dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(accs, columns=feats).mean().sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
